{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import haiku as hk\n",
    "import jax\n",
    "\n",
    "from collections import defaultdict\n",
    "from haiku._src.typing import Mapping\n",
    "from jax._src.basearray import Array\n",
    "\n",
    "from helpers import (\n",
    "    evaluate_transformer_decoder,\n",
    "    make_chomsky_generator,\n",
    "    make_model,\n",
    "    utm_data_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "ORDERED_TASKS = [\n",
    "    # Regular.\n",
    "    \"even_pairs\",\n",
    "    \"modular_arithmetic\",\n",
    "    \"parity_check\",\n",
    "    \"cycle_navigation\",\n",
    "    # Context free.\n",
    "    \"stack_manipulation\",\n",
    "    \"reverse_string\",\n",
    "    \"modular_arithmetic_brackets\",\n",
    "    \"solve_equation\",\n",
    "    # Context sensitive.\n",
    "    \"duplicate_string\",\n",
    "    \"missing_duplicate_string\",\n",
    "    \"odds_first\",\n",
    "    \"binary_addition\",\n",
    "    \"binary_multiplication\",\n",
    "    \"compute_sqrt\",\n",
    "    \"bucket_sort\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_params(\n",
    "    data_generator, params_path: str, batch_size: int = 128\n",
    ") -> hk.Params:\n",
    "    \"\"\"Loads saved model parameters and returns the initialized model and params.\n",
    "\n",
    "    Args:\n",
    "        params_path: Path to the saved .npz file containing model parameters\n",
    "        vocab_size: Size of the vocabulary used by the model\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing (model, params) where model is the initialized Haiku model\n",
    "        and params are the loaded parameters\n",
    "    \"\"\"\n",
    "    # Create the same model configuration as used in training\n",
    "    model = make_model(data_generator)\n",
    "\n",
    "    # Load the saved parameters\n",
    "    loaded = np.load(params_path, allow_pickle=True)\n",
    "    tree_def = loaded[\"tree_def\"].item()  # Get PyTreeDef\n",
    "    flat_params = [loaded[f\"arr_{i}\"] for i in range(len(loaded.files) - 1)]\n",
    "    loaded_params = jax.tree_util.tree_unflatten(tree_def, flat_params)\n",
    "\n",
    "    # Initialize the model with a dummy batch to get the parameter structure\n",
    "    dummy_batch, _ = data_generator.sample_dummy(batch_size)  # Minimal dummy input\n",
    "    dummy_batch = np.argmax(dummy_batch, axis=-1)\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    model.init(rng, dummy_batch)\n",
    "\n",
    "    return loaded_params\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"artifacts/params_markov_transformer_large.npz\", \n",
    "    # \"artifacts/params_original_transformer_large.npz\",\n",
    "    # \"artifacts/params_random_initialized_transformer_large.npz\",\n",
    "    # \"artifacts/params_markov_transformer_medium.npz\", \n",
    "    # \"artifacts/params_original_transformer_medium.npz\",\n",
    "    # \"artifacts/params_random_initialized_transformer_medium.npz\",\n",
    "    # \"artifacts/params_markov_transformer_small.npz\",\n",
    "    # \"artifacts/params_original_transformer_small.npz\",\n",
    "    # \"artifacts/params_random_initialized_transformer_small.npz\",\n",
    "]\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "utm_generator = utm_data_generator(rng)\n",
    "\n",
    "# Load the model and parameters\n",
    "model_params: Mapping[str, Mapping[str, Mapping[str, Array]]] = {}\n",
    "for model_path in model_paths:\n",
    "    params: Mapping[str, Mapping[str, Array]] = load_model_params(utm_generator, model_path)\n",
    "    model_params[model_path] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "even_pairs 0.47823447 0.47837195\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "modular_arithmetic 0.059472345 0.05948039\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "parity_check 0.4742412 0.47420883\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "cycle_navigation 0.30939212 0.3094097\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "stack_manipulation 0.34880376 0.29663175\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "reverse_string 0.4874289 0.4870159\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "modular_arithmetic_brackets 0.07442324 0.074425295\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "solve_equation 0.098978646 0.098978564\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "duplicate_string 0.47736844 0.47976524\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "missing_duplicate_string 0.46179977 0.4618\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "odds_first 0.4787263 0.4804724\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "binary_addition 0.4746074 0.47567868\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "binary_multiplication 0.4690976 0.46991166\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "Failed to evaluate task compute_sqrt empty range in randrange(1, 0)\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "bucket_sort 0.20277295 0.20503454\n"
     ]
    }
   ],
   "source": [
    "get_size_from_model_path = lambda path: \"large\" if \"large\" in path else \"medium\" if \"medium\" in path else \"small\" if \"small\" in path else \"unknown\"\n",
    "\n",
    "model_task_results = defaultdict(dict)# {model_str: {task: (regret, total_accuracy, total_final_accuracy)}}\n",
    "for model_str, params in model_params.items():\n",
    "    for task in ORDERED_TASKS:\n",
    "        print(\"Model: \", model_str)\n",
    "        try:\n",
    "            # max_input_length = 256 is arbitrary, I dont think this is explicitly defined in the paper.\n",
    "            chomsky_generator = make_chomsky_generator(\n",
    "                rng, task_str=task\n",
    "            )\n",
    "            regret, total_accuracy, total_final_accuracy = evaluate_transformer_decoder(\n",
    "                chomsky_generator, params, utm_generator, num_batches=10, size=get_size_from_model_path(model_str)\n",
    "            )\n",
    "            model_task_results[model_str][task] = (regret, total_accuracy, total_final_accuracy)\n",
    "            print(task, total_accuracy, total_final_accuracy)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to evaluate task\", task, e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
