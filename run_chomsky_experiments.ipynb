{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import haiku as hk\n",
    "import jax\n",
    "\n",
    "from haiku._src.typing import Mapping\n",
    "from jax._src.basearray import Array\n",
    "\n",
    "from helpers import (\n",
    "    evaluate_transformer_decoder,\n",
    "    make_chomsky_generator,\n",
    "    make_model,\n",
    "    utm_data_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "ORDERED_TASKS = [\n",
    "    # Regular.\n",
    "    \"even_pairs\",\n",
    "    \"modular_arithmetic\",\n",
    "    \"parity_check\",\n",
    "    \"cycle_navigation\",\n",
    "    # Context free.\n",
    "    \"stack_manipulation\",\n",
    "    \"reverse_string\",\n",
    "    \"modular_arithmetic_brackets\",\n",
    "    \"solve_equation\",\n",
    "    # Context sensitive.\n",
    "    \"duplicate_string\",\n",
    "    \"missing_duplicate_string\",\n",
    "    \"odds_first\",\n",
    "    \"binary_addition\",\n",
    "    \"binary_multiplication\",\n",
    "    \"compute_sqrt\",\n",
    "    \"bucket_sort\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_params(\n",
    "    data_generator, params_path: str, batch_size: int = 128\n",
    ") -> hk.Params:\n",
    "    \"\"\"Loads saved model parameters and returns the initialized model and params.\n",
    "\n",
    "    Args:\n",
    "        params_path: Path to the saved .npz file containing model parameters\n",
    "        vocab_size: Size of the vocabulary used by the model\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing (model, params) where model is the initialized Haiku model\n",
    "        and params are the loaded parameters\n",
    "    \"\"\"\n",
    "    # Create the same model configuration as used in training\n",
    "    model = make_model(data_generator)\n",
    "\n",
    "    # Load the saved parameters\n",
    "    loaded = np.load(params_path, allow_pickle=True)\n",
    "    tree_def = loaded[\"tree_def\"].item()  # Get PyTreeDef\n",
    "    flat_params = [loaded[f\"arr_{i}\"] for i in range(len(loaded.files) - 1)]\n",
    "    loaded_params = jax.tree_util.tree_unflatten(tree_def, flat_params)\n",
    "\n",
    "    # Initialize the model with a dummy batch to get the parameter structure\n",
    "    dummy_batch, _ = data_generator.sample_dummy(batch_size)  # Minimal dummy input\n",
    "    dummy_batch = np.argmax(dummy_batch, axis=-1)\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    model.init(rng, dummy_batch)\n",
    "\n",
    "    return loaded_params\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"artifacts/params_markov_transformer_large.npz\", \n",
    "    # \"artifacts/params_original_transformer_large.npz\",\n",
    "    # \"artifacts/params_random_initialized_transformer_large.npz\",\n",
    "    # \"artifacts/params_markov_transformer_medium.npz\", \n",
    "    # \"artifacts/params_original_transformer_medium.npz\",\n",
    "    # \"artifacts/params_random_initialized_transformer_medium.npz\",\n",
    "    # \"artifacts/params_markov_transformer_small.npz\",\n",
    "    # \"artifacts/params_original_transformer_small.npz\",\n",
    "    # \"artifacts/params_random_initialized_transformer_small.npz\",\n",
    "]\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "utm_generator = utm_data_generator(rng)\n",
    "\n",
    "# Load the model and parameters\n",
    "model_params: Mapping[str, Mapping[str, Mapping[str, Array]]] = {}\n",
    "for model_path in model_paths:\n",
    "    params: Mapping[str, Mapping[str, Array]] = load_model_params(utm_generator, model_path)\n",
    "    model_params[model_path] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "even_pairs 0.48148188 0.4815034\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "modular_arithmetic 0.058838494 0.058833234\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "parity_check 0.4728376 0.47282377\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "cycle_navigation 0.3117885 0.3117702\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "stack_manipulation 0.37461048 0.37046084\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "reverse_string 0.48154837 0.48626065\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "modular_arithmetic_brackets 0.07369607 0.073682815\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "solve_equation 0.09740067 0.097401455\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "duplicate_string 0.48207384 0.48070508\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "missing_duplicate_string 0.47244197 0.46932322\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "odds_first 0.47337928 0.47618812\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "binary_addition 0.474497 0.47003406\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "binary_multiplication 0.46480292 0.46319\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "Failed to evaluate task compute_sqrt empty range in randrange(1, 0)\n",
      "Model:  artifacts/params_markov_transformer_large.npz\n",
      "bucket_sort 0.20015983 0.1997672\n"
     ]
    }
   ],
   "source": [
    "get_size_from_model_path = lambda path: \"large\" if \"large\" in path else \"medium\" if \"medium\" in path else \"small\" if \"small\" in path else \"unknown\"\n",
    "for model_str, params in model_params.items():\n",
    "    for task in ORDERED_TASKS:\n",
    "        print(\"Model: \", model_str)\n",
    "        try:\n",
    "            # max_input_length = 256 is arbitrary, I dont think this is explicitly defined in the paper.\n",
    "            chomsky_generator = make_chomsky_generator(\n",
    "                rng, task_str=task\n",
    "            )\n",
    "            regret, total_accuracy, total_final_accuracy = evaluate_transformer_decoder(\n",
    "                chomsky_generator, params, utm_generator, num_batches=10, size=get_size_from_model_path(model_str)\n",
    "            )\n",
    "            print(task, total_accuracy, total_final_accuracy)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to evaluate task\", task, e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
