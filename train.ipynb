{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "import numpy as np\n",
    "import optax\n",
    "import tqdm\n",
    "import tree\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from absl import logging\n",
    "from typing import Any\n",
    "\n",
    "from data import utm_data_generator as utm_dg_lib\n",
    "from data import chomsky_data_generator as chomsky_sampler_lib\n",
    "from helpers import make_chomsky_generator, utm_data_generator, make_model, init_params, save_params, evaluate_transformer_decoder, CHOMSKY_ALPHABET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "# Follows the paper's parameters\n",
    "TRAINING_STEPS = 2000 # not really used in training loop\n",
    "EXECUTION_STEPS = 1000\n",
    "USE_DELIMITERS = True\n",
    "MEMORY_SIZE = 200\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_loss_fn(model: hk.Transformed) -> Any:\n",
    "  \"\"\"Returns the loss function for update_parameters.\"\"\"\n",
    "\n",
    "  def loss_fn(\n",
    "      params: hk.Params,\n",
    "      sequences: jax.Array,\n",
    "      mask: jax.Array,\n",
    "  ) -> jnp.float32:\n",
    "    \"\"\"Returns the loss for the model and the last state.\n",
    "\n",
    "    Args:\n",
    "      params: The parameters of the model, usually a neural network.\n",
    "      sequences: The input of sequences to evaluate. See neural_predictors.py.\n",
    "      mask: A binary array, True (1's) denote where to skip computing the loss.\n",
    "    \"\"\"\n",
    "    # This code computes the loss for a transformer decoder model:\n",
    "    # 1. Apply the model to get log probabilities (conditionals) for each token\n",
    "    conditionals = model.apply(\n",
    "        params=params,\n",
    "        targets=sequences,\n",
    "        rng=None,\n",
    "    )\n",
    "    # 2. Extract the log probabilities of the actual tokens that appeared in the sequence\n",
    "    # by using take_along_axis to select the probability corresponding to each token\n",
    "    true_conditionals = jnp.take_along_axis(\n",
    "        conditionals, sequences[..., None], axis=-1\n",
    "    )[..., 0]\n",
    "    # 3. Apply the mask to zero out log probabilities where we should skip computing loss (e.g., for padding tokens)\n",
    "    true_conditionals = jnp.where(mask, 0.0, true_conditionals)\n",
    "    # 4. Sum the log probabilities across the sequence dimension to get log likelihood per batch\n",
    "    marginals = jnp.sum(true_conditionals, axis=1)  # Shape (B,).\n",
    "    # 5. Return the negative mean log likelihood as the loss (for minimization)\n",
    "    return -jnp.mean(marginals)\n",
    "\n",
    "  return loss_fn\n",
    "\n",
    "\n",
    "@functools.partial(\n",
    "    jax.jit, static_argnames=('optimizer', 'grad_fn', 'normalize_gradients')\n",
    ")\n",
    "def _update_parameters(\n",
    "    params: hk.Params,\n",
    "    opt_state: optax.OptState,\n",
    "    sequences: jax.Array,\n",
    "    mask: jax.Array,\n",
    "    grad_fn: Any,\n",
    "    optimizer: optax.GradientTransformation,\n",
    "    normalize_gradients: bool = True,\n",
    ") -> tuple[hk.Params, optax.OptState, dict[str, Any]]:\n",
    "  \"\"\"Returns updated params and extra logs (like loss, last state etc).\n",
    "\n",
    "  Backpropagation is done on the whole sequence. The whole function is jitted.\n",
    "\n",
    "  Args:\n",
    "    params: The current parameters of the network.\n",
    "    opt_state: The optimizer state.\n",
    "    sequences: The input of sequences to evaluate. See base_predictor.py.\n",
    "    mask: A binary array, True (1's) denote where to skip computing the loss.\n",
    "    grad_fn: A gradient function, which takes some parameters, a random seed,\n",
    "      the data to compute the gradient on, and an initial state for the\n",
    "      predictor. It returns the gradient of the parameters for this batch of\n",
    "      data, and extra values.\n",
    "    optimizer: An optax optimizer.\n",
    "    normalize_gradients: Whether to divide the gradients by the length of the\n",
    "      sequences, or keep them as is. Using this option guarantees to have the\n",
    "      same scale across various sequence lengths, and therefore tasks.\n",
    "  \"\"\"\n",
    "  loss, grad = grad_fn(params, sequences, mask)\n",
    "  if normalize_gradients:\n",
    "    length_sequence = float(sequences.shape[1])\n",
    "    grad = tree.map_structure(lambda x: x / length_sequence, grad)\n",
    "  updates, new_opt_state = optimizer.update(grad, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "  log_dict = {\n",
    "      'loss': loss,\n",
    "      'grad_norm_unclipped': optax.global_norm(grad),\n",
    "  }\n",
    "\n",
    "  return new_params, new_opt_state, log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_decoder(\n",
    "    data_generator: utm_dg_lib.UTMDataGenerator,\n",
    "    training_steps: int,\n",
    "    log_every: int,\n",
    "    batch_size: int,\n",
    "    use_tqdm: bool = True,\n",
    "    with_markov: bool = False,\n",
    "    size: str = \"large\",\n",
    "    eval_data_generator: chomsky_sampler_lib.ChomskyDataGenerator = None,\n",
    ") -> tuple[hk.Params, float, list[float], list[float], list[float]]:\n",
    "    \"\"\"Trains a neural network on some synthetic data.\n",
    "\n",
    "    We train a decoder-only transformer on batches, minimizing the log-loss\n",
    "    objective. The exact architecture can be modified using the TransformerConfig\n",
    "    object (defined in models/transformer.py)\n",
    "\n",
    "    Args:\n",
    "      data_generator: Used to generate batches of data to train on.\n",
    "      training_steps: Number of batches to train on.\n",
    "      log_every: How often to log the loss. If negative or 0, no log at all.\n",
    "      batch_size: The number of sequences in a batch.\n",
    "      use_tqdm: Whether to use a progress bar or not.\n",
    "\n",
    "    Returns:\n",
    "      The final loss, and final parameters.\n",
    "    \"\"\"\n",
    "    print(\"Vocab Size:\", data_generator.feature_size)\n",
    "    print(\"Model Size:\", size)\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"With Markov:\", with_markov)\n",
    "    model = make_model(data_generator, size)\n",
    "\n",
    "    params = init_params(model, data_generator, batch_size)\n",
    "\n",
    "    # Make gradient function.\n",
    "    loss_fn = _make_loss_fn(model)\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=False)\n",
    "\n",
    "    # Make optimizer, to apply the gradients.\n",
    "    optimizer = optax.adam(learning_rate=1e-4)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    logging.info(\"Initialization done, starting training...\")\n",
    "\n",
    "    last_loss = 0.0\n",
    "    default_mask = lambda x: np.ones(x.shape[:2], dtype=bool)\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    eval_final_accs = []\n",
    "\n",
    "    for step in tqdm.trange(training_steps, disable=not use_tqdm):\n",
    "        batch, log_dict = data_generator.sample(with_markov=with_markov)\n",
    "        # Transform one-hots to integer tokens.\n",
    "        batch = np.argmax(batch, axis=-1)\n",
    "        if \"loss_mask\" in log_dict:\n",
    "            loss_mask = log_dict[\"loss_mask\"]\n",
    "        else:\n",
    "            loss_mask = default_mask(batch)\n",
    "\n",
    "        params, opt_state, logs = _update_parameters(\n",
    "            params=params,\n",
    "            opt_state=opt_state,\n",
    "            sequences=batch,\n",
    "            grad_fn=grad_fn,\n",
    "            optimizer=optimizer,\n",
    "            mask=loss_mask,\n",
    "        )\n",
    "\n",
    "        if log_every > 0 and step % log_every == 0:\n",
    "            logging.info(\n",
    "                \"Step %d, Loss (avg cumulative nats) %f, Grad norm %f\",\n",
    "                step,\n",
    "                logs[\"loss\"],\n",
    "                logs[\"grad_norm_unclipped\"],\n",
    "            )\n",
    "\n",
    "        if step % (log_every * 10) == 0 and eval_data_generator is not None:\n",
    "            last_loss = logs[\"loss\"]\n",
    "            eval_loss, eval_acc, eval_final_acc = evaluate_transformer_decoder(\n",
    "                eval_data_generator, params, data_generator, size=size\n",
    "            )\n",
    "            eval_losses.append(eval_loss)\n",
    "            eval_accs.append(eval_acc)\n",
    "            eval_final_accs.append(eval_final_acc)\n",
    "            print(\n",
    "                f\"Step {step}, Eval acc: {eval_acc}, Eval final acc: {eval_final_acc}\"\n",
    "            )\n",
    "\n",
    "    return params, last_loss, eval_losses, eval_accs, eval_final_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 128\n",
      "Model Size: small\n",
      "Batch Size: 32\n",
      "With Markov: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 128\n",
      "Model Size: medium\n",
      "Batch Size: 32\n",
      "With Markov: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 128\n",
      "Model Size: large\n",
      "Batch Size: 32\n",
      "With Markov: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(seed=1)\n",
    "\n",
    "utm_generator = utm_data_generator(\n",
    "    rng,\n",
    "    maximum_steps=EXECUTION_STEPS,\n",
    "    maximum_program_length=100,\n",
    "    memory_size=MEMORY_SIZE,\n",
    "    alphabet_size=CHOMSKY_ALPHABET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "chomsky_generator = make_chomsky_generator(rng, use_delimiters=USE_DELIMITERS)\n",
    "\n",
    "class SAMPLE_TYPE:\n",
    "    ORIGINAL = 0\n",
    "    MARKOV = 1\n",
    "    RANDOM = 2\n",
    "\n",
    "def training_loop():\n",
    "    model_sizes = [\n",
    "        \"small\",\n",
    "        \"medium\",\n",
    "        \"large\"\n",
    "    ]\n",
    "\n",
    "    sampling_types = [SAMPLE_TYPE.RANDOM, SAMPLE_TYPE.ORIGINAL, SAMPLE_TYPE.MARKOV]\n",
    "\n",
    "    training_steps_map = {\n",
    "        \"small\": 20000,\n",
    "        \"medium\": 8000,\n",
    "        \"large\": 2000\n",
    "    }\n",
    "\n",
    "    for sampling_type in sampling_types:\n",
    "        for model_size in model_sizes:\n",
    "            training_steps = training_steps_map[model_size]\n",
    "            if sampling_type == SAMPLE_TYPE.RANDOM:\n",
    "                training_steps = 0\n",
    "            params, loss, eval_losses, eval_accs, eval_final_accs = train_transformer_decoder(\n",
    "                data_generator=utm_generator,\n",
    "                training_steps=training_steps,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                log_every=10,\n",
    "                with_markov=sampling_type == SAMPLE_TYPE.MARKOV,\n",
    "                size=model_size,\n",
    "                eval_data_generator=chomsky_generator,\n",
    "            )\n",
    "            logging.info(\"Final loss: %f\", loss)\n",
    "\n",
    "            if sampling_type == SAMPLE_TYPE.ORIGINAL:\n",
    "                SUFFIX = 'original'\n",
    "            elif sampling_type == SAMPLE_TYPE.MARKOV:\n",
    "                SUFFIX = 'markov'\n",
    "            else:\n",
    "                SUFFIX = 'random'\n",
    "\n",
    "            file_name = f\"params_{SUFFIX}_transformer_{model_size}_steps_{training_steps}.npz\"\n",
    "            save_params(params, file_name)\n",
    "\n",
    "            logging.info(f\"Parameters saved in file {file_name}\")\n",
    "\n",
    "            # Create a pandas DataFrame from the evaluation metrics\n",
    "            eval_data = {\n",
    "                \"eval_losses\": eval_losses,\n",
    "                \"eval_accs\": eval_accs,\n",
    "                \"eval_final_accs\": eval_final_accs,\n",
    "            }\n",
    "            eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "            # Save the DataFrame to a CSV file\n",
    "            metrics_name = f\"metrics_{SUFFIX}_transformer_{model_size}_steps_{training_steps}.csv\"\n",
    "            if sampling_type != SAMPLE_TYPE.RANDOM:\n",
    "                eval_df.to_csv(metrics_name, index=False)\n",
    "\n",
    "            logging.info(f\"Evaluation metrics saved to {metrics_name}\")\n",
    "\n",
    "training_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
